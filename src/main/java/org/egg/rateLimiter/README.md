
总纲：

由于是漏桶算法，此工具不适用于有突发量的场景，更适用于平缓处理的逻辑，比如清洗数据场景，API消费者限速场景；API提供者限速不合适；

问题：

本地限速器，无法解决分布式下流量不均匀。





解决方案：

关键点：负载均衡，本地限速，效率高效，可控可靠
角色：客户端(sdk)、控制端（redis）
核心功能点：
sdk定时推送指标数据（详见下列指标说明）到redis中；
sdk定时刷新客户端的本地限速器上限max


指标说明：

max	客户端的限速器QPS上限
curr	客户端当前的QPS
avg	客户端单位时间内的qps平均值


权重说明：

根据客户端的avg来给限流加权，加权公式：

A_max=m*(A_avg/(A_avg+B_avg))

其中A_max代表客户端A的max指标，m代表设置的分布式总QPS指标，A_avg代表客户端A的avg指标；B_avg代表客户端B的avg指标



待解决问题：

机器重启或发布，机器未注册完全的情况下，如何设置限流值
拉取redis指标数据，如果没有，初始化数据结构后本机默认max=m;
如果已有数据，根据已有数据和本机进行权重计算，算法为新机器从其他台机器上偷取限流量，其他机器按照自身的权重给与新机器；公式为：新机器C_max=(1/（已有机器数量+1）)*A_max+(1/（已有机器数量+1）)*B_max+... 
如果配置强行m限制，则新机器需要等其他机器的max更新后才能启动成功，否则不受阻；可以通过受阻一定时间来当做其他机器已同步最新max；或者，判断其他机器ip的version指标是否都发送变更；后续可以优化成推模式，让其他机器刷新max，推模式太重了，暂不考虑
机器缩容宕机情况下，需要支持探活功能，移除掉废弃机器IP的数据
与新增机器相反，需要缩容的机器的指标在分配给其他机器，但是机器不一定能优雅的shutdown，故可以采取其他机器主动同步计算权重指标时慢慢吞噬掉这些指标 公式和加权公式一样：A_max=m*(A_avg/(A_avg+B_avg))
debug模式下，如何便于测试组件可用性
需要新增监控，用于观察组件最大qps，客户端avg等指标数据
每次更新指标后，推送监控打点即可
设计：如何确保不超过最大值m？因为只有拉模式，客户端之前数据是有很大时差的，可能会出现所有机器max累计之后抽过m的情况；以防万一，可以新增如下逻辑：
计算本机max值后，需要累计所有机器的max总和，如果大于m,则使用m-其他机器max;


数据结构：

redis key的定义：appId_sourceKey 比如：100023008_washHrCashData 每个redis key互相隔离

value定义 ：使用hash,其中key为客户端ip,value为指标信息 {"max":100,"curr":10,"avg":6,"version":1}







如何落地：

起一个本地线程定时刷新，先拉取redis的各客户端的avg计算去本机的max,调整成功后，在推送本机的原来的max，avg指标；
初期新增开关，支持降级，降级方案为：监控发现不符合预期内容时，可以关闭自动负载均衡逻辑，所有机器平分m;新增机器会超过m;
落地效果：

压测效果：



竞品比较：

和redis比较减去了网络开销（本地0.05ms,redis 1ms+），无热key问题；